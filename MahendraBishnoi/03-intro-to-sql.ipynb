{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Getting Started with SQL and BigQuery\n[Tutorial Link](https://www.kaggle.com/dansbecker/getting-started-with-sql-and-bigquery) <br/>\n[Notebook Link](https://www.kaggle.com/mahendrabishnoi2/03-intro-to-sql/) on kaggle.\n\n## Introduction\n- **SQL**(Structured Query Language) - A programming language used with databases. <br>\n- **BigQuery** - A web service that lets us apply SQL to huge datasets.\n\nIn this tutorial we will learn about accessing and examining BigQuery datasets.\n\n## First BigQuery commands\nTo use BigQuery we will import the Python package."},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"from google.cloud import bigquery","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"First step in the workflow is to create a `Client` objct. `Client` object plays a central role in retrieving information from bigquery datasets."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a client object\nclient = bigquery.Client()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will work with dataset of posts on [Hacker News](https://news.ycombinator.com/)\n\nIn BigQuery each dataset in contained in a corresponding project. In this case `hacker_news` dataset is contained in the `bigquery-public-data` project. To access the dataset,\n- Construct a reference to the dataset by using `dataset()` method.\n- Next, use the `get_dataset()` method, along with the reference we just constructed, to fetch the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Construct a reference to the \"hacker_news\" dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# construct a reference to the hacker_news dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each dataset is a collection of tables.\n\nWe use `list_tables()` method to list tables present in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"# List tables present in \"hacker_news\" dataset\ntables = list(client.list_tables(dataset))\n\n# print names of all tables present in the dataset\nfor table in tables:\n    print(table.table_id)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Similar to how we fetched a dataset, we can fetch a table. In the code below we fetch the `full` table in the `hacker_news` dataset using `get_table()` method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a table reference (\"full\")\ntable_ref = dataset_ref.table(\"full\")\n\n# API request - fetch the full table\ntable = client.get_table(table_ref)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"What we have learnt so far:\n<img src=\"https://i.imgur.com/biYqbUB.png\"/>\n\n## Table Schema\nStructure of a table is called its schema. We need to understand a table's schema to pull the data we want.\n\nHere we will investigate the `full` table that we fetched earlier."},{"metadata":{"trusted":true},"cell_type":"code","source":"table.schema","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Each `SchemaField` tells us about a specific column (also referred to as `field`). In order the information is:\n- The **name** of the column\n- The **field type** (or **data type**) of the column\n- The **mode** of the column (`NULLABLE` means the column allows NULL values, default)\n- A **description** of the data in that column\n\nThe first **field** has the `SchemaField`:\n`SchemaField('by', 'STRING', 'NULLABLE', \"The username of the item's author.\", ()),` <br/>\nThis tells us:\n- The **field** (or **column**) is called `by`,\n- The data in the field is strings,\n- This column allow NULL values,\n- This column contains username of item's author.\n\nWe can use `list_rows()` method to show first five rows of the `full` table to make sure its right. This returns a BigQuery `RowIterator` object which can be converted to pandas DataFrame using `to_dataframe()` method."},{"metadata":{"trusted":true},"cell_type":"code","source":"# print first five rows of the \"full\" table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Above code printed out first five rows of all **fields**, we can also print first five rows of selected fields if we want. For example here we will print first five rows of `by` **field**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# print first five rows of \"by\" column (or field) of \"full\" table\nclient.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Setup**\n- import `from google.cloud import bigquery`\n- create a client object `client = bigquery.Client()\n\n**Fetching Dataset**\n- create a reference to dataset `dataset_ref = client.dataset(\"dataset_name\", project=\"project_name\")`\n- fetch dataset `dataset = client.get_dataset(dataset_ref)\n\n**List all tables present in dataset**\n- list tables `tables = list(client.list_tables(dataset))` `print(table[0].table_id)`\n\n**Fetch a table from dataset**\n- create reference to a table `table_ref = dataset_ref.table(\"table_name\")`\n- fetch table `table = client.get_table(table_ref)`\n\n**Schema**\n- schema of a table `table.schema`\n\n**List rows**\n- all rows `client.list_rows(table, max_results=5).to_dataframe()`\n- specific columns `client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()`"},{"metadata":{},"cell_type":"markdown","source":"Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-getting-started-with-sql-and-bigquery)"},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{},"cell_type":"markdown","source":"# SELECT, FROM & WHERE\n[Tutorial Link](https://www.kaggle.com/dansbecker/select-from-where)\n\nNow that we know how to access and examine a dataset, we will start writing SQL queries. SQL queries help us sort through massive datasets, to retrieve only the information we want.\n\nIn this tutorial we will **SELECT**, **FROM** and **WHERE** to get data from specific columns based on specified conditions.\n\nWe will work with an small imaginary dataset `pet_records` which contains just one table, called `pets`.\n<img src=\"https://i.imgur.com/fI5Pvvp.png\"/>\n\n## SELECT ... FROM\nThe most basic SQL query selects single column from a table. To do this \n- specify the column you want after the word **SELECT**, and then\n- specify the table after the word **FROM**\n\nTo select the `Name` column (from the `pets` table, in the `pet_records` databse in the `bigquery-public-data` project), our query would appear as follows:\n<img src=\"https://i.imgur.com/c3GxYRt.png\"/>\n\n> Note: When writing a SQL query, the argument passed to **FROM** is not in single or double quotation marks (' or \"). Its in backticks (\\`).\n\n## WHERE\nBigQuery datasets are huge, so we'll usually want to return rows meeting specific conditions. We can do so using the **WHERE** clause.\n\nThe query below returns the entries from the `name` column that are in rows where the `Animal` column has the text `Cat`.\n<img src=\"https://i.imgur.com/HJOT8Kb.png\"/>\n"},{"metadata":{},"cell_type":"markdown","source":"## Example: Where all the the US citites in the OpenAQ dataset?\nWe'll use [OpenAQ](https://openaq.org/) dataset about air quality. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports \nfrom google.cloud import bigquery\n\n# create a client object \nclient = bigquery.Client()\n\n# create a dataset reference (to openaq)\ndataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n\n# api request - fetch data\ndataset = client.get_dataset(dataset_ref)\n\n# create a list of tables present in the dataset\ntables = list(client.list_tables(dataset))\n\n# print all table names\nfor table in tables:\n    print(table.table_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# construct a reference to the table\ntable_ref = dataset_ref.table(\"global_air_quality\")\n\n# api request - fetch table\ntable = client.get_table(table_ref)\n\n# print first five rows / lines of \"global_air_quality\" table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Write a query to select `city` column from `global_air_quality` table where `country` is `US`"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\nquery = \"\"\"\n        SELECT city\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country = 'US'\n        \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submitting the query to the dataset\n- Create a client object"},{"metadata":{"trusted":true},"cell_type":"code","source":"client = bigquery.Client()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we set up the query with the `query()` method. We will run the method with the default parameters but this method allows us to specify complicated settings as shown in [documentation](https://google-cloud.readthedocs.io/en/latest/bigquery/generated/google.cloud.bigquery.client.Client.query.html#google.cloud.bigquery.client.Client.query)."},{"metadata":{"trusted":true},"cell_type":"code","source":"query_job = client.query(query)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next we run the query and convert results to a pandas dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# api request - run the query and return a pandas DataFrame\nus_cities = query_job.to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"us_cities.city.value_counts().head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## More queries\nIf we want multiple columns we can select them with comma"},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"\"\"\n        SELECT city, country\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country=\"US\"\n        \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"If we want to select all columns we can use `*`"},{"metadata":{"trusted":true},"cell_type":"code","source":"query = \"\"\"\n        SELECT *\n        FROM `bigquery-public-data.openaq.global_air_quality`\n        WHERE country=\"US\"\n        \"\"\"","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Working with Big Datasets\nTo avoid scanning too much data at once, we can estimate size of query before we run it. We will see how to estimate query size on very large `hacker_news` dataset.\n\nTo see how much data a query will scan, we create a `QueryJobConfig` object and set the `dry_run` parameter to `True`."},{"metadata":{"trusted":true},"cell_type":"code","source":"# query to get the score column from every row where the type column has value \"job\"\nquery = \"\"\"\n        SELECT score, title\n        FROM `bigquery-public-data.hacker_news.full`\n        WHERE type = \"job\" \n        \"\"\"\n\n# create 'QueryJobConfig' object to estimate size of query without running it\ndry_run_config = bigquery.QueryJobConfig(dry_run=True)\n\n# api request - dry run query to estimate costs\ndry_run_query_job = client.query(query, job_config=dry_run_config)\n\nprint(\"This query will process {} bytes\".format(dry_run_query_job.total_bytes_processed))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can also specify a parameter when running the query to limit how much data we want to scan. Here's an example with a low limit."},{"metadata":{"trusted":true},"cell_type":"code","source":"max_size = 1 \nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=max_size)\n\n# setup the query (only run if its less than 100 MB)\nsafe_query_job = client.query(query, job_config=safe_config)\n\n# api request - run the query and return a pandas dataframe\nsafe_query_job.to_dataframe()\n\n\n# strangely this query runs (need to take a look on documentation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Only run the query if it's less than 1 GB\nONE_GB = 1000*1000*1000\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n\n# Set up the query (will only run if it's less than 1 GB)\nsafe_query_job = client.query(query, job_config=safe_config)\n\n# API request - try to run the query, and return a pandas DataFrame\njob_post_scores = safe_query_job.to_dataframe()\n\n# Print average score for job posts\njob_post_scores.score.mean()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-select-from-where)"},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# Group By, Having & Count\n[Tutorial Link](https://www.kaggle.com/dansbecker/group-by-having-count)\n\nNow that we know how to select raw data, we will learn how to group selected data and count things within those groups to answer questions like: \n- How many of each kind of fruit has our store sold?\n- How many species of animal has the vet office treated?\n\n"},{"metadata":{},"cell_type":"markdown","source":"In this tutorial we will learn about **GROUP BY**, **HAVING** & **COUNT()**. We will be using following made-up table to understand these techniques\n<img src=\"https://i.imgur.com/fI5Pvvp.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"## COUNT()\n**COUNT()**, as the name suggests return the count of things. If we pass name of a column to **COUNT()**, it will return number of items in that column.\n\nFor example if we **SELECT** the **COUNT()** of `ID` columns in `pets` table, it will return 4, because there are 4 IDs.\n<img src=\"https://i.imgur.com/Eu5HkXq.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"## GROUP BY\n**GROUP BY** takes the name of one or more columns, and treats all rows with the same value in that column as a single group when you apply aggregate functions like **COUNT()**.\n\nFor example, say we want to know how many of each type of animal we have in the `pets` table. We can use **GROUP BY** to group together rows that have the same value in the `Animal` column, while using **COUNT()** to find out how many ID's we have in each group.\n<img src=\"https://i.imgur.com/tqE9Eh8.png\"/>\nIt returns a table with three rows (one for each distinct animal). We can see that the `pets` table contains 1 rabbit, 1 dog, and 2 cats."},{"metadata":{},"cell_type":"markdown","source":"## GROUP BY ... HAVING\n**HAVING** is used in combination with **GROUP BY** to ignore groups that don't meet certain criteria.\n\nSo this query, for example, will only include groups that have more than one ID in them.\n<img src=\"https://i.imgur.com/2ImXfHQ.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"## Example: Which Hacker News comments generated the most discussion?\nWe will work with `comments` table."},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports \nfrom google.cloud import bigquery\n\n# create a client object\nclient = bigquery.Client()\n\n# construct a reference to the hacker_news dataset\ndataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n\n# api request - fetch dataset\ndataset = client.get_dataset(dataset_ref)\n\n# list tables in the 'hacker_news' dataset\ntables = list(client.list_tables(dataset))\n\n# print all tables present in the dataset\nfor table in tables:\n    print(table.table_id)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# construct a reference to 'comments' table\ntable_ref = dataset_ref.table('comments')\n\n# api request - fetch dataset\ntable = client.get_table(table_ref)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"client.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From above table we can say that:\n- id column shows id of each comment\n- parent column shows the comment (id) that was replied to \n\nSo we can **GROUP BY** the `parent` and **COUNT()** the `id` column to figure out the number of replies for every comment, and since we are looking for popular comments we can apply a condition using **HAVING** to return only those comments which received more than 10 replies."},{"metadata":{"trusted":true},"cell_type":"code","source":"# query to select comments that have more than 10 replies\nquery = \"\"\"\n        SELECT parent, COUNT(id)\n        FROM `bigquery-public-data.hacker_news.comments`\n        GROUP BY parent\n        HAVING COUNT(id) > 10\n        \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up query and QueryJobConfig (To be on safer side)\nsafe_config = bigquery.QueryJobConfig(max_bytes_billed = 10*9)       # 1 GB limit\nquery_job = client.query(query, job_config=safe_config)\n\n# api request - run the query and return a pandas dataframe\npopular_comments = query_job.to_dataframe()\n\n# print first five rows of popular_comments dataframe\npopular_comments.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Aliasing and Other Improvements\n- The column resulting from **COUNT(id)** was called `f0__`. That's not a very descriptive name. We can change the name by adding `AS NumPosts` after we specify the aggregation.\n- If we are ever unsure what to put inside the **COUNT()** function, we can do **COUNT(1)** to count the rows in each group. Most people find it especially readable, because we know it's not focusing on other columns. It also scans less data than if supplied column names (making it faster and using less of our data access quota(BigQuery 30 TB quota))."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Imporoved version of earlier query with aliasing and improved readability\nquery_improved = \"\"\"\n                 SELECT parent, COUNT(1) AS NumPosts\n                 FROM `bigquery-public-data.hacker_news.comments`\n                 GROUP BY parent\n                 HAVING COUNT(1) > 10\n                 \"\"\"\n\n# set up query and QueryJobConfig (To be on safer side)\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9) \nquery_job = client.query(query_improved, job_config=safe_config)\n\n# api request - run the query and return a pandas dataframe\nimproved_df = query_job.to_dataframe()\n\n# print first five rows of improved_df dataframe\nimproved_df.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Note on using **GROUP BY**\nNote that because it tells SQL how to apply aggregate functions (like **COUNT()**), it doesn't make sense to use **GROUP BY** without an aggregate function. Similarly, if we have any **GROUP BY** clause, then all variables must be passed to either a\n1. GROUP BY command, or\n2. an aggregation function."},{"metadata":{"trusted":true},"cell_type":"code","source":"query_good = \"\"\"\n             SELECT parent, COUNT(id)\n             FROM `bigquery-public-data.hacker_news.comments`\n             GROUP BY parent\n             \"\"\"\n\n# Example of a good query as parent is used with GROUP BY and id is used with COUNT()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"query_bad = \"\"\"\n            SELECT author, parent, COUNT(id)\n            FROM `bigquery-public-data.hacker_news.comments`\n            GROUP BY parent\n            \"\"\"\n\n# This query will throw an error because author is neither aggregated nor used with GROUP BY","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-group-by-having-count)"},{"metadata":{},"cell_type":"markdown","source":"# Order By\n[Tutorial Link](https://www.kaggle.com/dansbecker/order-by)\n\nIn this tutorial we will learn how to change order of our results using the **ORDER BY** clause and explore a popular use case by applying ordering to dates. We will use slightly modified version of `pets` table to understand **ORDER BY** clause.\n<img src=\"https://i.imgur.com/b99zTLv.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"## ORDER BY\n**ORDER BY** is usually the last clause in our query, used to sort the results returned by rest of our query. \n\nIn the above table we can see that `ID` column is not sorted, we can sort it by following query:\n<img src=\"https://i.imgur.com/6o9LuTA.png\"/>\n\n**ORDER BY** also works on columns containing text(strings), it will sort them alphabetically. Example:\n<img src=\"https://i.imgur.com/ooxuzw3.png\"/>\n\nWe can use **DESC** to reverse the sorting order as shown below:\n<img src=\"https://i.imgur.com/IElLJrR.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"## Dates\nThere are two ways dates can be stored in BigQuery as a **DATE** or as **DATETIME**\n\nThe **DATE** format has the year first, then month and then day. It looks like this:\n\n``YYYY-[M]M-[D]D``\n* `YYYY`: Four digit year\n* `[M]M`: One or two digit month\n* `[D]D`: One or two digit day\n\nSo `2019-01-10` is interpreted as January 10, 2019.\n\nThe **DATETIME** format is like **DATE** with time added at the end."},{"metadata":{},"cell_type":"markdown","source":"## EXTRACT\nIf we want to look at a part of **DATE** such as month or day or year, we can do so with **EXTRACT**. We will show use of **EXTRACT** with this slightly modified table, called `pets_with_date`\n<img src=\"https://i.imgur.com/vhvHIh0.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"The query below returns two columns, where `Day` column contains the day corresponding to each entry in the `Date` column.\n<img src=\"https://i.imgur.com/PhoWBO0.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"Following query returns one column with just the week in the year for each date in the `Date` column.\n<img src=\"https://i.imgur.com/A5hqGxY.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"We can find all the functions we can use with dates in BigQuery in [this documentation](https://cloud.google.com/bigquery/docs/reference/legacy-sql#datetimefunctions)"},{"metadata":{},"cell_type":"markdown","source":"## Example: Which day of the week has the most fatal motor accidents?\nWe'll investigate the `accident_2015` table from US Traffic Fatality Records database, which contains information on traffic accidents in the US where at least one person died."},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports\nfrom google.cloud import bigquery\n\n# create a client object\nclient = bigquery.Client()\n\n# construct a reference to the 'nhtsa_traffic_fatalities' database\ndataset_ref = client.dataset(\"nhtsa_traffic_fatalities\", project=\"bigquery-public-data\")\n\n# api request - fetch dataset\ndataset = client.get_dataset(dataset_ref)\n\n# construct a reference to the 'accident_2015' table\ntable_ref = dataset_ref.table(\"accident_2015\")\n\n# api request - fetch the table\ntable = client.get_table(table_ref)\n\n# show first five rows of the table fetched\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* `consecutive_number` column contains unique id for each accident\n* `timestamp_of_crash` contains the date of accident in **DATETIME** format\n\nWe can\n* **EXTRACT** the day of the week (as `day_of_week`) from `timestamp_of_crash` and\n* **GROUP BY** the day of the week, before we **COUNT** the `consecutive_number` column to determine number of accidents on each day of the week. "},{"metadata":{"trusted":true},"cell_type":"code","source":"# query to find number of accidents on each day of the week\nquery = \"\"\"\n        SELECT EXTRACT(DAYOFWEEK from timestamp_of_crash) AS day_of_week,\n               COUNT(consecutive_number) AS num_accidents\n        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n        GROUP BY day_of_week\n        ORDER BY num_accidents DESC\n        \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# set up query and QueryJobConfig\nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9)\nquery_job = client.query(query, job_config=safe_config)\n\n# run query and convert the result to a dataframe\ndf = query_job.to_dataframe()\n\ndf","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-order-by)"},{"metadata":{},"cell_type":"markdown","source":" "},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# AS & WITH\nHelps organize our queries for better readability. Important when working with complex queries. To understand **AS** & **WITH** we will use `pets` table, which now includes age of animals.\n<img src=\"https://i.imgur.com/MXrsiAZ.png\"/>\n\n## AS\nAs we have seen earlier **AS** is used to rename the columns generated by queries, also known as **aliasing**. Its similar to how we use `as` in Python. Eg: `import pandas as pd`. Here we kind of renamed `pandas` to `pd`. To use **AS** we insert it just after column name."},{"metadata":{},"cell_type":"markdown","source":"A SQL query without **AS**\n<img src=\"https://i.imgur.com/VelX9tP.png\"/>\n\nSame query with **AS**\n<img src=\"https://i.imgur.com/teF84tU.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"## WITH ... AS\n**AS** when combined with **WITH** is called CTE (Common Table Expression).\n\nCTE is a temporary table that we return within our query. Helpful in splitting our queries in readable chunks and we can write queries against them.\n\nFor example, if we want to ask questions from `pets` table about older animals in particular. We could write a CTE which only contains information about animals older than 5 years.\n<img src=\"https://i.imgur.com/0Kz8q4x.png\"/>\n\nAbove query is incomplete, so it doesn't return anything but it creates a CTE named `Seniors` which we can refer to while writing rest of the query.\n\nWe can finish the query by pulling the information we want from CTE named `Seniors`. Below query first creates CTE then return `Id` column from CTE.\n<img src=\"https://i.imgur.com/3xQZM4p.png\"/>"},{"metadata":{},"cell_type":"markdown","source":"Also, it's important to note that CTEs only exist inside the query where we create them, and we can't reference them in later queries. So, any query that uses a CTE is always broken into two parts: (1) first, we create the CTE, and then (2) we write a query that uses the CTE."},{"metadata":{},"cell_type":"markdown","source":"## Example: How many Bitcoin transactions are made per month?"},{"metadata":{"trusted":true},"cell_type":"code","source":"from google.cloud import bigquery\n\n# Create a \"Client\" object\nclient = bigquery.Client()\n\n# Construct a reference to the \"crypto_bitcoin\" dataset\ndataset_ref = client.dataset(\"crypto_bitcoin\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\n# Construct a reference to the \"transactions\" table\ntable_ref = dataset_ref.table(\"transactions\")\n\n# API request - fetch the table\ntable = client.get_table(table_ref)\n\n# Preview the first five lines of the \"transactions\" table\nclient.list_rows(table, max_results=5).to_dataframe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`block_timestamp` column contains date of each transaction in DATETIME format, we will convert it into DATE format using **DATE()**.\n\nWe will do that using CTE, then in next part we will count the number of transactions per month and sort them so earlier dates appear first."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Query to select number of transactions per date, sorted by data\nquery = \"\"\"\n        WITH time AS\n        (\n        SELECT DATE(block_timestamp) AS trans_date\n        FROM `bigquery-public-data.crypto_bitcoin.transactions`\n        )\n        SELECT COUNT(1) AS transactions,\n               trans_date\n        FROM time\n        GROUP BY trans_date\n        ORDER BY trans_date\n        \"\"\"","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-output":false,"trusted":true},"cell_type":"code","source":"# set up query \nsafe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\nquery_job = client.query(query, job_config=safe_config)\n\n# run query return dataframe\ndf = query_job.to_dataframe()\n\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since they're returned sorted, we can easily plot the raw results to show us the number of Bitcoin transactions per day over the whole timespan of this dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"df.set_index('trans_date').plot()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-as-with)"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}
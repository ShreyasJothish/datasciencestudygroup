{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Getting Started with SQL and BigQuery\n",
    "[Tutorial Link](https://www.kaggle.com/dansbecker/getting-started-with-sql-and-bigquery) <br/>\n",
    "[Notebook Link](https://www.kaggle.com/mahendrabishnoi2/03-intro-to-sql/) on kaggle.\n",
    "\n",
    "## Introduction\n",
    "- **SQL**(Structured Query Language) - A programming language used with databases. <br>\n",
    "- **BigQuery** - A web service that lets us apply SQL to huge datasets.\n",
    "\n",
    "In this tutorial we will learn about accessing and examining BigQuery datasets.\n",
    "\n",
    "## First BigQuery commands\n",
    "To use BigQuery we will import the Python package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step in the workflow is to create a `Client` objct. `Client` object plays a central role in retrieving information from bigquery datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a client object\n",
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with dataset of posts on [Hacker News](https://news.ycombinator.com/)\n",
    "\n",
    "In BigQuery each dataset in contained in a corresponding project. In this case `hacker_news` dataset is contained in the `bigquery-public-data` project. To access the dataset,\n",
    "- Construct a reference to the dataset by using `dataset()` method.\n",
    "- Next, use the `get_dataset()` method, along with the reference we just constructed, to fetch the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a reference to the \"hacker_news\" dataset\n",
    "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a reference to the hacker_news dataset\n",
    "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
    "\n",
    "# API request - fetch the dataset\n",
    "dataset = client.get_dataset(dataset_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is a collection of tables.\n",
    "\n",
    "We use `list_tables()` method to list tables present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List tables present in \"hacker_news\" dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "\n",
    "# print names of all tables present in the dataset\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how we fetched a dataset, we can fetch a table. In the code below we fetch the `full` table in the `hacker_news` dataset using `get_table()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table reference (\"full\")\n",
    "table_ref = dataset_ref.table(\"full\")\n",
    "\n",
    "# API request - fetch the full table\n",
    "table = client.get_table(table_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have learnt so far:\n",
    "<img src=\"https://i.imgur.com/biYqbUB.png\"/>\n",
    "\n",
    "## Table Schema\n",
    "Structure of a table is called its schema. We need to understand a table's schema to pull the data we want.\n",
    "\n",
    "Here we will investigate the `full` table that we fetched earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each `SchemaField` tells us about a specific column (also referred to as `field`). In order the information is:\n",
    "- The **name** of the column\n",
    "- The **field type** (or **data type**) of the column\n",
    "- The **mode** of the column (`NULLABLE` means the column allows NULL values, default)\n",
    "- A **description** of the data in that column\n",
    "\n",
    "The first **field** has the `SchemaField`:\n",
    "`SchemaField('by', 'STRING', 'NULLABLE', \"The username of the item's author.\", ()),` <br/>\n",
    "This tells us:\n",
    "- The **field** (or **column**) is called `by`,\n",
    "- The data in the field is strings,\n",
    "- This column allow NULL values,\n",
    "- This column contains username of item's author.\n",
    "\n",
    "We can use `list_rows()` method to show first five rows of the `full` table to make sure its right. This returns a BigQuery `RowIterator` object which can be converted to pandas DataFrame using `to_dataframe()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first five rows of the \"full\" table\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above code printed out first five rows of all **fields**, we can also print first five rows of selected fields if we want. For example here we will print first five rows of `by` **field**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print first five rows of \"by\" column (or field) of \"full\" table\n",
    "client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setup**\n",
    "- import `from google.cloud import bigquery`\n",
    "- create a client object `client = bigquery.Client()\n",
    "\n",
    "**Fetching Dataset**\n",
    "- create a reference to dataset `dataset_ref = client.dataset(\"dataset_name\", project=\"project_name\")`\n",
    "- fetch dataset `dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "**List all tables present in dataset**\n",
    "- list tables `tables = list(client.list_tables(dataset))` `print(table[0].table_id)`\n",
    "\n",
    "**Fetch a table from dataset**\n",
    "- create reference to a table `table_ref = dataset_ref.table(\"table_name\")`\n",
    "- fetch table `table = client.get_table(table_ref)`\n",
    "\n",
    "**Schema**\n",
    "- schema of a table `table.schema`\n",
    "\n",
    "**List rows**\n",
    "- all rows `client.list_rows(table, max_results=5).to_dataframe()`\n",
    "- specific columns `client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-getting-started-with-sql-and-bigquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SELECT, FROM & WHERE\n",
    "[Tutorial Link](https://www.kaggle.com/dansbecker/select-from-where)\n",
    "\n",
    "Now that we know how to access and examine a dataset, we will start writing SQL queries. SQL queries help us sort through massive datasets, to retrieve only the information we want.\n",
    "\n",
    "In this tutorial we will **SELECT**, **FROM** and **WHERE** to get data from specific columns based on specified conditions.\n",
    "\n",
    "We will work with an small imaginary dataset `pet_records` which contains just one table, called `pets`.\n",
    "<img src=\"https://i.imgur.com/fI5Pvvp.png\"/>\n",
    "\n",
    "## SELECT ... FROM\n",
    "The most basic SQL query selects single column from a table. To do this \n",
    "- specify the column you want after the word **SELECT**, and then\n",
    "- specify the table after the word **FROM**\n",
    "\n",
    "To select the `Name` column (from the `pets` table, in the `pet_records` databse in the `bigquery-public-data` project), our query would appear as follows:\n",
    "<img src=\"https://i.imgur.com/c3GxYRt.png\"/>\n",
    "\n",
    "> Note: When writing a SQL query, the argument passed to **FROM** is not in single or double quotation marks (' or \"). Its in backticks (\\`).\n",
    "\n",
    "## WHERE\n",
    "BigQuery datasets are huge, so we'll usually want to return rows meeting specific conditions. We can do so using the **WHERE** clause.\n",
    "\n",
    "The query below returns the entries from the `name` column that are in rows where the `Animal` column has the text `Cat`.\n",
    "<img src=\"https://i.imgur.com/HJOT8Kb.png\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Where all the the US citites in the OpenAQ dataset?\n",
    "We'll use [OpenAQ](https://openaq.org/) dataset about air quality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from google.cloud import bigquery\n",
    "\n",
    "# create a client object \n",
    "client = bigquery.Client()\n",
    "\n",
    "# create a dataset reference (to openaq)\n",
    "dataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n",
    "\n",
    "# api request - fetch data\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# create a list of tables present in the dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "\n",
    "# print all table names\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a reference to the table\n",
    "table_ref = dataset_ref.table(\"global_air_quality\")\n",
    "\n",
    "# api request - fetch table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# print first five rows / lines of \"global_air_quality\" table\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a query to select `city` column from `global_air_quality` table where `country` is `US`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n",
    "query = \"\"\"\n",
    "        SELECT city\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting the query to the dataset\n",
    "- Create a client object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = bigquery.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we set up the query with the `query()` method. We will run the method with the default parameters but this method allows us to specify complicated settings as shown in [documentation](https://google-cloud.readthedocs.io/en/latest/bigquery/generated/google.cloud.bigquery.client.Client.query.html#google.cloud.bigquery.client.Client.query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_job = client.query(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we run the query and convert results to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api request - run the query and return a pandas DataFrame\n",
    "us_cities = query_job.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_cities.city.value_counts().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More queries\n",
    "If we want multiple columns we can select them with comma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT city, country\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country=\"US\"\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to select all columns we can use `*`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country=\"US\"\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Big Datasets\n",
    "To avoid scanning too much data at once, we can estimate size of query before we run it. We will see how to estimate query size on very large `hacker_news` dataset.\n",
    "\n",
    "To see how much data a query will scan, we create a `QueryJobConfig` object and set the `dry_run` parameter to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to get the score column from every row where the type column has value \"job\"\n",
    "query = \"\"\"\n",
    "        SELECT score, title\n",
    "        FROM `bigquery-public-data.hacker_news.full`\n",
    "        WHERE type = \"job\" \n",
    "        \"\"\"\n",
    "\n",
    "# create 'QueryJobConfig' object to estimate size of query without running it\n",
    "dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "\n",
    "# api request - dry run query to estimate costs\n",
    "dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
    "\n",
    "print(\"This query will process {} bytes\".format(dry_run_query_job.total_bytes_processed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify a parameter when running the query to limit how much data we want to scan. Here's an example with a low limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 1 \n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=max_size)\n",
    "\n",
    "# setup the query (only run if its less than 100 MB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# api request - run the query and return a pandas dataframe\n",
    "safe_query_job.to_dataframe()\n",
    "\n",
    "\n",
    "# strangely this query runs (need to take a look on documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run the query if it's less than 1 GB\n",
    "ONE_GB = 1000*1000*1000\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n",
    "\n",
    "# Set up the query (will only run if it's less than 1 GB)\n",
    "safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# API request - try to run the query, and return a pandas DataFrame\n",
    "job_post_scores = safe_query_job.to_dataframe()\n",
    "\n",
    "# Print average score for job posts\n",
    "job_post_scores.score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-select-from-where)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group By, Having & Count\n",
    "[Tutorial Link](https://www.kaggle.com/dansbecker/group-by-having-count)\n",
    "\n",
    "Now that we know how to select raw data, we will learn how to group selected data and count things within those groups to answer questions like: \n",
    "- How many of each kind of fruit has our store sold?\n",
    "- How many species of animal has the vet office treated?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we will learn about **GROUP BY**, **HAVING** & **COUNT()**. We will be using following made-up table to understand these techniques\n",
    "<img src=\"https://i.imgur.com/fI5Pvvp.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COUNT()\n",
    "**COUNT()**, as the name suggests return the count of things. If we pass name of a column to **COUNT()**, it will return number of items in that column.\n",
    "\n",
    "For example if we **SELECT** the **COUNT()** of `ID` columns in `pets` table, it will return 4, because there are 4 IDs.\n",
    "<img src=\"https://i.imgur.com/Eu5HkXq.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUP BY\n",
    "**GROUP BY** takes the name of one or more columns, and treats all rows with the same value in that column as a single group when you apply aggregate functions like **COUNT()**.\n",
    "\n",
    "For example, say we want to know how many of each type of animal we have in the `pets` table. We can use **GROUP BY** to group together rows that have the same value in the `Animal` column, while using **COUNT()** to find out how many ID's we have in each group.\n",
    "<img src=\"https://i.imgur.com/tqE9Eh8.png\"/>\n",
    "It returns a table with three rows (one for each distinct animal). We can see that the `pets` table contains 1 rabbit, 1 dog, and 2 cats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUP BY ... HAVING\n",
    "**HAVING** is used in combination with **GROUP BY** to ignore groups that don't meet certain criteria.\n",
    "\n",
    "So this query, for example, will only include groups that have more than one ID in them.\n",
    "<img src=\"https://i.imgur.com/2ImXfHQ.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Which Hacker News comments generated the most discussion?\n",
    "We will work with `comments` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from google.cloud import bigquery\n",
    "\n",
    "# create a client object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# construct a reference to the hacker_news dataset\n",
    "dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
    "\n",
    "# api request - fetch dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# list tables in the 'hacker_news' dataset\n",
    "tables = list(client.list_tables(dataset))\n",
    "\n",
    "# print all tables present in the dataset\n",
    "for table in tables:\n",
    "    print(table.table_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a reference to 'comments' table\n",
    "table_ref = dataset_ref.table('comments')\n",
    "\n",
    "# api request - fetch dataset\n",
    "table = client.get_table(table_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above table we can say that:\n",
    "- id column shows id of each comment\n",
    "- parent column shows the comment (id) that was replied to \n",
    "\n",
    "So we can **GROUP BY** the `parent` and **COUNT()** the `id` column to figure out the number of replies for every comment, and since we are looking for popular comments we can apply a condition using **HAVING** to return only those comments which received more than 10 replies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to select comments that have more than 10 replies\n",
    "query = \"\"\"\n",
    "        SELECT parent, COUNT(id)\n",
    "        FROM `bigquery-public-data.hacker_news.comments`\n",
    "        GROUP BY parent\n",
    "        HAVING COUNT(id) > 10\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query and QueryJobConfig (To be on safer side)\n",
    "safe_config = bigquery.QueryJobConfig(max_bytes_billed = 10*9)       # 1 GB limit\n",
    "query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# api request - run the query and return a pandas dataframe\n",
    "popular_comments = query_job.to_dataframe()\n",
    "\n",
    "# print first five rows of popular_comments dataframe\n",
    "popular_comments.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aliasing and Other Improvements\n",
    "- The column resulting from **COUNT(id)** was called `f0__`. That's not a very descriptive name. We can change the name by adding `AS NumPosts` after we specify the aggregation.\n",
    "- If we are ever unsure what to put inside the **COUNT()** function, we can do **COUNT(1)** to count the rows in each group. Most people find it especially readable, because we know it's not focusing on other columns. It also scans less data than if supplied column names (making it faster and using less of our data access quota(BigQuery 30 TB quota))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imporoved version of earlier query with aliasing and improved readability\n",
    "query_improved = \"\"\"\n",
    "                 SELECT parent, COUNT(1) AS NumPosts\n",
    "                 FROM `bigquery-public-data.hacker_news.comments`\n",
    "                 GROUP BY parent\n",
    "                 HAVING COUNT(1) > 10\n",
    "                 \"\"\"\n",
    "\n",
    "# set up query and QueryJobConfig (To be on safer side)\n",
    "safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**9) \n",
    "query_job = client.query(query_improved, job_config=safe_config)\n",
    "\n",
    "# api request - run the query and return a pandas dataframe\n",
    "improved_df = query_job.to_dataframe()\n",
    "\n",
    "# print first five rows of improved_df dataframe\n",
    "improved_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note on using **GROUP BY**\n",
    "Note that because it tells SQL how to apply aggregate functions (like **COUNT()**), it doesn't make sense to use **GROUP BY** without an aggregate function. Similarly, if we have any **GROUP BY** clause, then all variables must be passed to either a\n",
    "1. GROUP BY command, or\n",
    "2. an aggregation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_good = \"\"\"\n",
    "             SELECT parent, COUNT(id)\n",
    "             FROM `bigquery-public-data.hacker_news.comments`\n",
    "             GROUP BY parent\n",
    "             \"\"\"\n",
    "\n",
    "# Example of a good query as parent is used with GROUP BY and id is used with COUNT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bad = \"\"\"\n",
    "            SELECT author, parent, COUNT(id)\n",
    "            FROM `bigquery-public-data.hacker_news.comments`\n",
    "            GROUP BY parent\n",
    "            \"\"\"\n",
    "\n",
    "# This query will throw an error because author is neither aggregated nor used with GROUP BY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-group-by-having-count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order By\n",
    "[Tutorial Link](https://www.kaggle.com/dansbecker/order-by)\n",
    "\n",
    "In this tutorial we will learn how to change order of our results using the **ORDER BY** clause and explore a popular use case by applying ordering to dates. We will use slightly modified version of `pets` table to understand **ORDER BY** clause.\n",
    "<img src=\"https://i.imgur.com/b99zTLv.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ORDER BY\n",
    "**ORDER BY** is usually the last clause in our query, used to sort the results returned by rest of our query. \n",
    "\n",
    "In the above table we can see that `ID` column is not sorted, we can sort it by following query:\n",
    "<img src=\"https://i.imgur.com/6o9LuTA.png\"/>\n",
    "\n",
    "**ORDER BY** also works on columns containing text(strings), it will sort them alphabetically. Example:\n",
    "<img src=\"https://i.imgur.com/ooxuzw3.png\"/>\n",
    "\n",
    "We can use **DESC** to reverse the sorting order as shown below:\n",
    "<img src=\"https://i.imgur.com/IElLJrR.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates\n",
    "There are two ways dates can be stored in BigQuery as a **DATE** or as **DATETIME**\n",
    "\n",
    "The **DATE** format has the year first, then month and then day. It looks like this:\n",
    "\n",
    "``YYYY-[M]M-[D]D``\n",
    "* `YYYY`: Four digit year\n",
    "* `[M]M`: One or two digit month\n",
    "* `[D]D`: One or two digit day\n",
    "\n",
    "So `2019-01-10` is interpreted as January 10, 2019.\n",
    "\n",
    "The **DATETIME** format is like **DATE** with time added at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT\n",
    "If we want to look at a part of **DATE** such as month or day or year, we can do so with **EXTRACT**. We will show use of **EXTRACT** with this slightly modified table, called `pets_with_date`\n",
    "<img src=\"https://i.imgur.com/vhvHIh0.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The query below returns two columns, where `Day` column contains the day corresponding to each entry in the `Date` column.\n",
    "<img src=\"https://i.imgur.com/PhoWBO0.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following query returns one column with just the week in the year for each date in the `Date` column.\n",
    "<img src=\"https://i.imgur.com/A5hqGxY.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find all the functions we can use with dates in BigQuery in [this documentation](https://cloud.google.com/bigquery/docs/reference/legacy-sql#datetimefunctions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Which day of the week has the most fatal motor accidents?\n",
    "We'll investigate the `accident_2015` table from US Traffic Fatality Records database, which contains information on traffic accidents in the US where at least one person died."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from google.cloud import bigquery\n",
    "\n",
    "# create a client object\n",
    "client = bigquery.Client()\n",
    "\n",
    "# construct a reference to the 'nhtsa_traffic_fatalities' database\n",
    "dataset_ref = client.dataset(\"nhtsa_traffic_fatalities\", project=\"bigquery-public-data\")\n",
    "\n",
    "# api request - fetch dataset\n",
    "dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "# construct a reference to the 'accident_2015' table\n",
    "table_ref = dataset_ref.table(\"accident_2015\")\n",
    "\n",
    "# api request - fetch the table\n",
    "table = client.get_table(table_ref)\n",
    "\n",
    "# show first five rows of the table fetched\n",
    "client.list_rows(table, max_results=5).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `consecutive_number` column contains unique id for each accident\n",
    "* `timestamp_of_crash` contains the date of accident in **DATETIME** format\n",
    "\n",
    "We can\n",
    "* **EXTRACT** the day of the week (as `day_of_week`) from `timestamp_of_crash` and\n",
    "* **GROUP BY** the day of the week, before we **COUNT** the `consecutive_number` column to determine number of accidents on each day of the week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query to find number of accidents on each day of the week\n",
    "query = \"\"\"\n",
    "        SELECT EXTRACT(DAYOFWEEK from timestamp_of_crash) AS day_of_week,\n",
    "               COUNT(consecutive_number) AS num_accidents\n",
    "        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n",
    "        GROUP BY day_of_week\n",
    "        ORDER BY num_accidents DESC\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up query and QueryJobConfig\n",
    "safe_config = bigquery.QueryJobConfig(max_bytes_billed=10**9)\n",
    "query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "# run query and convert the result to a dataframe\n",
    "df = query_job.to_dataframe()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises of this tutorial are solved [here](https://www.kaggle.com/mahendrabishnoi2/exercise-order-by)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
